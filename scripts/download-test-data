#!/usr/bin/env python
"""
Kevin Murphy
keggsmurph21 at gmail dot com
"""

from mypy_boto3_s3 import Client as S3Client

import boto3  # type: ignore
import botocore  # type: ignore
import json
import os
import shutil
import subprocess
import sys
import tarfile


BUCKET = "swatphonlab"
PREFIX = "ultratrace-test-data"


def usage() -> None:
    print(
        f"""
usage: {sys.argv[0]}

    This script downloads and extracts tarballs from an AWS S3
    bucket into the ./test-data folder.  In order to run this
    script, the following environment variables must be set and
    exported:

     - AWS_ACCESS_KEY_ID
     - AWS_SECRET_ACCESS_KEY

    To set and export these variables, you can run the following
    command from the root of the project repository:

     $ source .env

    If that file does not exist on your system, please contact
    one of the project maintainers for a copy.

    Further, the IAM role associated with the credentials must have
    the correct permissions (READ and/or WRITE) on the S3 bucket.
""",
        file=sys.stderr,
    )
    sys.exit(1)


def hash_path(path: str) -> str:
    assert os.path.exists(path)
    proc = subprocess.run(
        ["./scripts/hash-path", path], stdout=subprocess.PIPE, encoding="utf-8"
    )
    if proc.returncode:
        raise RuntimeError("Unable to hash path")
    return proc.stdout.strip()


def download_file(s3: S3Client, key: str, local_path: str) -> None:
    print(f"downloading s3://{BUCKET}/{key} ...", file=sys.stderr)
    try:
        s3.download_file(
            Bucket=BUCKET, Key=key, Filename=local_path,
        )
    except botocore.exceptions.ClientError as e:
        print(repr(e), file=sys.stderr)
        usage()


def check_hash(local_path: str, expected_hash: str) -> bool:
    return os.path.exists(local_path) and hash_path(local_path) == expected_hash


if __name__ == "__main__":

    if os.environ.get("AWS_ACCESS_KEY_ID", None) is None:
        usage()
    if os.environ.get("AWS_SECRET_ACCESS_KEY", None) is None:
        usage()

    repo_root = os.path.realpath(os.path.join(os.path.dirname(sys.argv[0]), "..",))

    os.chdir(repo_root)

    s3 = boto3.client("s3")

    os.makedirs("./test-data", exist_ok=True)

    download_file(s3, f"{PREFIX}/manifest.json", "./test-data/manifest.json")
    with open("./test-data/manifest.json") as fp:
        manifest = json.load(fp)

    for dataset_name, dataset_metadata in manifest["datasets"].items():
        local_path = f"./test-data/{dataset_name}"
        dataset_version = int(dataset_metadata["version"])
        dataset_hash = dataset_metadata["hash"]
        if os.path.exists(local_path):
            if check_hash(local_path, dataset_hash):
                print("hash matches expected value, skipping ...", file=sys.stderr)
                continue
            print(
                "hash doesn't match expected value, pulling latest archive ...",
                file=sys.stderr,
            )
            shutil.rmtree(local_path)
        local_archive_path = f"{local_path}.tar.gz"
        download_file(
            s3,
            f"{PREFIX}/{dataset_name}/v{dataset_version}.tar.gz",
            local_archive_path,
        )
        with tarfile.open(local_archive_path, "r:gz") as tf:
            tf.extractall("./test-data")
            assert check_hash(local_path, dataset_hash)
        os.unlink(local_archive_path)

    print("done!", file=sys.stderr)
