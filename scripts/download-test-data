#!/bin/bash
#
# Kevin Murphy
# keggsmurph21 at gmail dot com

set -euo pipefail

usage() {
    cat <<EOM

usage: $0

    This script downloads and extracts tarballs from an AWS S3
    bucket into the ./test-data folder.  In order to run this
    script, the following environment variables must be set and
    exported:

     - AWS_ACCESS_KEY_ID
     - AWS_SECRET_ACCESS_KEY

    To set and export these variables, you can run the following
    command from the root of the project repository:

     $ source .env

    If that file does not exist on your system, please contact
    one of the project maintainers for a copy.

EOM
    exit 1
}

# Helper functions to hash an entire directory, so that we can check if
# contents match some known expected value (see below).
get_hash() {
    hash_func=md5sum  # not sure how portable this is
    echo "$@" | md5sum | cut -d' ' -f1
}

# For each file in the directory, use `get_hash` to compute a hash of
# its contents, then concat that hash to the `running_hash` string. Then,
# when each of the individual files has been hashed, use `get_hash` to
# echo out the hash of the `running_hash` string.
hash_dir() {
    echo "computing hash for $1 ..." >&2
    running_hash=""
    for file in $(find "$1" -type f | sort); do
        running_hash="${running_hash}$(get_hash "$file")"
    done
    get_hash $running_hash
}

# This is the output from
#
#   $ hash_dir $REPO/test-data
#
# when all of the required files are present and in the right place.
# If we run this command and it has a different output from the value
# below, that means we are missing files and should reconstruct the
# test data from the S3 tarballs.
#
# This value should be updated every time we make updates to ./test-data.
MAGIC_HASH="326e33e80882799e088c0f34fb39aa8f"

# Download each of these dataset tarballs (space-delimited)
DATASETS="ftyers qumuq"

# enforce that all environment variables are set
[ -z "${AWS_ACCESS_KEY_ID:-}" ] && usage
[ -z "${AWS_SECRET_ACCESS_KEY:-}" ] && usage

# go to the repository root
cd "$(dirname "$0")/.."

# see if we need to download anything
if [ -d ./test-data ]; then
    current_hash=$(hash_dir ./test-data)
    if [[ $current_hash == $MAGIC_HASH ]]; then
        echo "hash matches expected value, exiting!" >&2
        exit 0
    else
        echo "hash doesn't match expected value, pulling latest data ..." >&2
        rm -rf ./test-data
    fi
fi

mkdir ./test-data

# download the files using python AWS SDK
cat <<DOWNLOAD | python3

import boto3
import sys

BUCKET = "swatphonlab"
PREFIX = "ultratrace-test-data"

s3 = boto3.client("s3")

for dataset in "$DATASETS".split():
    tarball = f"{PREFIX}/{dataset}.tar.gz"
    print(
        f"downloading s3://{BUCKET}/{tarball} ...",
        file=sys.stderr,
    )
    s3.download_file(
        Bucket=BUCKET,
        Key=tarball,
        Filename=f"./test-data/{dataset}.tar.gz",
    )

DOWNLOAD

cd ./test-data

# extract tarballs
for dataset in $DATASETS; do
    echo "extracting ./$dataset.tar.gz ..." >&2
    tar xf "./$dataset.tar.gz"
    rm "./$dataset.tar.gz"
done

cd ..

echo "done!" >&2

# FIXME: Build up some more trivial test directories
